# Awesome-MMLM

## 相关数据

| 序号 | 名称 | 数据分类 | 语言 | 下载地址 | 发布机构 | 相关链接 | 简介 |
| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: | :--: | :--: |
| 1 | LLaVA-Pretrain | Pretrain | EN | [🤗](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain) / [🤖](https://www.modelscope.cn/datasets/thomas/LLaVA-Pretrain/summary) | [haotian-liu](https://github.com/haotian-liu) | [HomePage](https://llava-vl.github.io) / [Paper](https://arxiv.org/abs/2310.03744) / [GitHub](https://github.com/haotian-liu/LLaVA) | LLaVA Visual Instruct Pretrain LCS-558K 是 LAION/CC/SBU 数据集的一个子集，经过筛选的概念覆盖分布更为均衡。该数据集是为视觉指令调整中的特征对齐预训练阶段而构建的。目标是建立面向 GPT-4 视觉/语言能力的大型多模态。 |
| 2 | LLaVA-Instruct-150K | Instruct | EN| [🤗](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) / [🤖](https://www.modelscope.cn/datasets/thomas/LLaVA-Instruct-150K/files) | [haotian-liu](https://github.com/haotian-liu) | [HomePage](https://llava-vl.github.io) / [Paper](https://arxiv.org/abs/2310.03744) / [GitHub](https://github.com/haotian-liu/LLaVA) | LLaVA Visual Instruct 150K 是一套由 GPT 生成的多模态指令跟踪数据。它是为视觉指令调整和建立面向 GPT-4 视觉/语言能力的大型多模态而构建的。 |
| 3 | ShareGPT4V | PreTrain & Instruct | EN | [🤗](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V) / [🤖](https://www.modelscope.cn/datasets/thomas/ShareGPT4V/summary) | [LinChen](https://github.com/xiaoachen98) | [HomePage](https://sharegpt4v.github.io) / [paper](https://arxiv.org/abs/2311.12793) / [GitHub](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V) | ShareGPT4V Captions 1.2M 是一组由 GPT4-Vision 支持的多模态字幕数据。其构建目的是为了在预训练和监督微调阶段增强大型多模态模型（LMM）的模态对齐和细粒度视觉概念感知能力。这一进步旨在使 LMM 达到 GPT4-Vision 的能力。 |
| 4 | ViP-LLaVA-Instruct | Instruct | EN | [🤗](https://huggingface.co/datasets/mucai/ViP-LLaVA-Instruct) / [🤖](https://www.modelscope.cn/datasets/thomas/ViP-LLaVA-Instruct/summary) | [mu-cai](https://github.com/mu-cai) | [HomePage](https://vip-llava.github.io) / [Paper](https://arxiv.org/abs/2312.00784) / [GitHub](https://github.com/WisconsinAIVision/ViP-LLaVA) | ViP-LLaVA Instruct由LLaVA-1.5指令数据和区域级视觉提示数据组成。它的构造用于视觉指令调优和构建大型多式联运，以实现GPT-4级区域理解能力。 |
| 5 | MoE-LLaVA | EN | PreTrain & Instruct | [🤗](https://huggingface.co/datasets/LanguageBind/MoE-LLaVA) / [🤖](https://www.modelscope.cn/datasets/thomas/MoE-LLaVA/summary) | [LanguageBind](https://github.com/LinB203) | [Paper](https://arxiv.org/abs/2401.15947) / [GitHub](https://github.com/PKU-YuanGroup/MoE-LLaVA) | / |
| 6 | AS-V2 | EN | PreTrain & Instruct | [🤗](https://huggingface.co/datasets/OpenGVLab/AS-V2) / [🤖](https://www.modelscope.cn/datasets/thomas/AS-V2/summary) | [OpenGVLab](https://github.com/OpenGVLab) | [Paper](https://arxiv.org/abs/2402.19474) / [GitHub](https://github.com/OpenGVLab/all-seeing) | 提出了一项新任务，称为 "关系对话（ReC）"，它将文本生成、对象定位和关系理解统一起来。在统一表述的基础上，我们构建了由 127K 个高质量关系对话样本组成的 AS-V2 数据集，以释放多模态大语言模型（MLLM）的 ReC 能力。 |
| 7 | llava-plus-data | EN | Instruct | [🤗](https://huggingface.co/datasets/LLaVA-VL/llava-plus-data) / [🤖](https://www.modelscope.cn/datasets/thomas/llava-plus-data/summary) | [LLaVA-VL](https://github.com/LLaVA-VL) | [HomePage](https://llava-vl.github.io/llava-plus/) / [Paper](https://arxiv.org/abs/2311.05437) / [GitHub](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase) | LLaVA-Plus-v1-117K 是一套由 GPT 生成的多模态工具增强指令跟随数据。它是为构建具有 GPT-4-plus 视觉/语言能力的多模态大模型而构建的，于2023年9月通过提示ChatGPT/GPT-4-0314 API收集。 |
| 8 | ALLaVA-4V | EN | PreTrain & Instruct | [🤗](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V) / [🤖](https://www.modelscope.cn/datasets/thomas/ALLaVA-4V/summary) | [FreedomIntelligence](https://github.com/FreedomIntelligence) | [Paper](https://arxiv.org/abs/2402.11684) / [GitHub](https://github.com/FreedomIntelligence/ALLaVA) | / |
| 9 | ALLaVA-4V-Chinses | CN | PreTrain & Instruct | [🤗](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V-Chinese) / [🤖](https://www.modelscope.cn/datasets/thomas/ALLaVA-4V-Chinese/summary) | [FreedomIntelligence](https://github.com/FreedomIntelligence) | [Paper](https://arxiv.org/abs/2402.11684) / [GitHub](https://github.com/FreedomIntelligence/ALLaVA) | / |
| 10 | the_cauldron | EN | PreTrain & Instruct | [🤗](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) / [🤖](https://www.modelscope.cn/datasets/thomas/the_cauldron/summary) | [HuggingFaceM4](https://huggingface.co/HuggingFaceM4) | / | 这是50个视觉语言数据集（仅限训练集）的大量集合，用于微调视觉语言模型Idefics2。 |
| 11 | UniMM-Chat | EN | Instruct | [🤗](https://huggingface.co/datasets/Yirany/UniMM-Chat) / [🤖](https://www.modelscope.cn/datasets/thomas/UniMM-Chat/summary) | [thunlp](https://github.com/thunlp) | [Paper](https://arxiv.org/abs/2310.00653) / [GitHub](https://github.com/thunlp/muffin) | UniMM-Chat数据集是由GPT-3.5提供支持的开源、知识密集型和多轮多模式对话数据，利用来自不同VL数据集的补充注释，并使用GPT-3.5生成与每个图像对应的多轮对话，产生117,238个对话，平均每个对话9.89个回合。 |
| 12 | SA1B-Dense-Caption | Pretrain | CN | [🤖](https://www.modelscope.cn/datasets/Tongyi-DataEngine/SA1B-Dense-Caption/summary) | [Tongyi-DataEngine](https://www.modelscope.cn/organization/Tongyi-DataEngine) | / | sa-1b数据集中的8631528张图片的详细的、高质量的、长文本描述。该数据集的图片描述分为整体描述和细节元素两方面，其中细节描述为图片中的重点元素的描述，整体 述包含细节描述汇总各种信息 |
| 13 | Layout-Instruction-Data | Pretrain & Instruct | EN |  [🤖](https://www.modelscope.cn/datasets/iic/Layout-Instruction-Data/summary) | [AlibabaResearch](https://github.com/AlibabaResearch) |  [Paper](https://arxiv.org/abs/2404.05225) / [GitHub](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/783504f985b100267e9ecab42cabfddd3b026a4f/DocumentUnderstanding/LayoutLLM) | 布局指令调优数据 |
| 14 | DocReason25K | Instruct | EN | [🤗](https://huggingface.co/datasets/mPLUG/DocReason25K) / [🤖](https://www.modelscope.cn/datasets/iic/DocReason25K/summary) | [X-PLUG](https://github.com/X-PLUG) | [Paper](https://arxiv.org/abs/2403.12895) / [GitHub](https://github.com/X-PLUG/mPLUG-DocOwl) | DocReason25K是mPLUG-DocOwl1.5-Chat使用的文档领域带有详细推理解释的指令微调训练集。 DocReason25K中的问题来源于DocVQA, InfographicsVQA, WikiTableQuestions, VisualMRC, ChartQA以及TextVQA。 DocReason25K中的详细推理解释由GPT3.5或GPT4V产生，并通过和人工标注的简单回复进行对比来过滤错误的答案。 |
| 15 | DocStruct4M | Pretrain & Instruct | EN | [🤗](https://huggingface.co/datasets/mPLUG/DocStruct4M) / [🤖](https://www.modelscope.cn/datasets/iic/DocStruct4M/summary) | [X-PLUG](https://github.com/X-PLUG) | [Paper](https://arxiv.org/abs/2403.12895) / [GitHub](https://github.com/X-PLUG/mPLUG-DocOwl) | DocStruct4M是多模态文档大模型mPLUG-DocOwl1.5在“统一文档结构学习”阶段的训练集，覆盖文档图片、网页图片、表格、图表和自然图。 它包含大概3M"结构化解析"训练样例以及1M"多粒度文字定位和识别"样例。 |
| 16 | DocDownstream | Pretrain & Instruct  | EN | [🤗](https://huggingface.co/datasets/mPLUG/DocDownstream-1.0) / [🤖](https://www.modelscope.cn/datasets/iic/DocDownstream-1.0/summary) | [X-PLUG](https://github.com/X-PLUG) | [Paper](https://arxiv.org/abs/2403.12895) / [GitHub](https://github.com/X-PLUG/mPLUG-DocOwl) | DocDownstream-1.0整合了10个文档图片理解数据集，包括DocVQA, InfographicsVQA, DeepForm, KleisterCharity, WikiTableQuestions, TabFact, ChartQA, TextCaps, TextVQA以及VisualMRC。 任务包括信息抽取、视觉问答、自然语言推理以及图片描述等。DocDownstream-1.0将所有任务统一为问答的形式。 |
| 17 | DocLocal4K | Pretrain & Instruct | EN | [🤗](https://huggingface.co/datasets/mPLUG/DocLocal4K) / [🤖](https://www.modelscope.cn/datasets/iic/DocLocal4K/summary) | [X-PLUG](https://github.com/X-PLUG) | [Paper](https://arxiv.org/abs/2403.12895) / [GitHub](https://github.com/X-PLUG/mPLUG-DocOwl) | / |


## 相关项目

## Benchmark

## 推理框架